{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f1f241",
   "metadata": {},
   "source": [
    "## Location Modelling\n",
    "### Address extraction, Address parsing, and Geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55e52053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import codecs, json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cfc064f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting address\n",
      "  Downloading address-0.1.1.tar.gz (110 kB)\n",
      "     ------------------------------------ 110.8/110.8 kB 143.0 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: address\n",
      "  Building wheel for address (setup.py): started\n",
      "  Building wheel for address (setup.py): finished with status 'done'\n",
      "  Created wheel for address: filename=address-0.1.1-py3-none-any.whl size=110197 sha256=7b8b2dc279bea9092944612a25d6701643040cc27d74ce999179bb0d3e6fd645\n",
      "  Stored in directory: c:\\users\\abhinaya\\appdata\\local\\pip\\cache\\wheels\\39\\5b\\74\\1f7461ca954bab5c4a6c9f0734bfc86e3ba31580080db4d515\n",
      "Successfully built address\n",
      "Installing collected packages: address\n",
      "Successfully installed address-0.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b5aa95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from address_parser import Parser\n",
    "import usaddress\n",
    "#from postal.parser import parse_address\n",
    "#from postal.expand import expand_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a33fcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting censusdata\n",
      "  Downloading CensusData-1.15.post1.tar.gz (26.6 MB)\n",
      "     -------------------------------------- 26.6/26.6 MB 435.2 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pandas in c:\\users\\abhinaya\\anaconda3\\lib\\site-packages (from censusdata) (1.4.4)\n",
      "Requirement already satisfied: requests in c:\\users\\abhinaya\\anaconda3\\lib\\site-packages (from censusdata) (2.28.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\abhinaya\\anaconda3\\lib\\site-packages (from pandas->censusdata) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\abhinaya\\anaconda3\\lib\\site-packages (from pandas->censusdata) (1.21.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\abhinaya\\anaconda3\\lib\\site-packages (from pandas->censusdata) (2022.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\abhinaya\\anaconda3\\lib\\site-packages (from requests->censusdata) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhinaya\\anaconda3\\lib\\site-packages (from requests->censusdata) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\abhinaya\\anaconda3\\lib\\site-packages (from requests->censusdata) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abhinaya\\anaconda3\\lib\\site-packages (from requests->censusdata) (3.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\abhinaya\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->censusdata) (1.16.0)\n",
      "Building wheels for collected packages: censusdata\n",
      "  Building wheel for censusdata (setup.py): started\n",
      "  Building wheel for censusdata (setup.py): finished with status 'done'\n",
      "  Created wheel for censusdata: filename=CensusData-1.15.post1-py3-none-any.whl size=28205746 sha256=dc1332a44f60160699032b476a0b5694fedd82e8975e2ae5d355aed72c7405a3\n",
      "  Stored in directory: c:\\users\\abhinaya\\appdata\\local\\pip\\cache\\wheels\\54\\96\\84\\245773d5290c5bb024ff9c3d80fc5466eefed704b4136cfe85\n",
      "Successfully built censusdata\n",
      "Installing collected packages: censusdata\n",
      "Successfully installed censusdata-1.15.post1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install censusdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91b22c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting census\n",
      "  Downloading census-0.8.19-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: requests>=1.1.0 in c:\\users\\abhinaya\\anaconda3\\lib\\site-packages (from census) (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\abhinaya\\anaconda3\\lib\\site-packages (from requests>=1.1.0->census) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abhinaya\\anaconda3\\lib\\site-packages (from requests>=1.1.0->census) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\abhinaya\\anaconda3\\lib\\site-packages (from requests>=1.1.0->census) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhinaya\\anaconda3\\lib\\site-packages (from requests>=1.1.0->census) (2022.9.14)\n",
      "Installing collected packages: census\n",
      "Successfully installed census-0.8.19\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "943a5d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting usNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading us-2.0.2.tar.gz (14 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jellyfish==0.6.1\n",
      "  Downloading jellyfish-0.6.1.tar.gz (132 kB)\n",
      "     ------------------------------------ 132.6/132.6 kB 118.6 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: us, jellyfish\n",
      "  Building wheel for us (setup.py): started\n",
      "  Building wheel for us (setup.py): finished with status 'done'\n",
      "  Created wheel for us: filename=us-2.0.2-py3-none-any.whl size=11940 sha256=7af8f5b3ee8cb2822d2cca69e96bd8c9c648a94f13f2d7e1636d3f42fe2f7450\n",
      "  Stored in directory: c:\\users\\abhinaya\\appdata\\local\\pip\\cache\\wheels\\1a\\93\\5b\\98d3861ec2c4a9d90b16324c6f8d7e4db03e6a830bc993adbb\n",
      "  Building wheel for jellyfish (setup.py): started\n",
      "  Building wheel for jellyfish (setup.py): finished with status 'done'\n",
      "  Created wheel for jellyfish: filename=jellyfish-0.6.1-py3-none-any.whl size=10369 sha256=c0979223fb666b55e62060b1af1ea5533562116f8efece6599ddaf2abf32cc75\n",
      "  Stored in directory: c:\\users\\abhinaya\\appdata\\local\\pip\\cache\\wheels\\e6\\7d\\be\\a937dbd1f988778a15011a563ac3a12917103bfc25ff6cb473\n",
      "Successfully built us jellyfish\n",
      "Installing collected packages: jellyfish, us\n",
      "  Attempting uninstall: jellyfish\n",
      "    Found existing installation: jellyfish 0.9.0\n",
      "    Uninstalling jellyfish-0.9.0:\n",
      "      Successfully uninstalled jellyfish-0.9.0\n",
      "Successfully installed jellyfish-0.6.1 us-2.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.2.2 requires pyqtwebengine<5.13, which is not installed.\n",
      "spyder 5.2.2 requires jellyfish>=0.7, but you have jellyfish 0.6.1 which is incompatible.\n",
      "spyder 5.2.2 requires pyqt5<5.13, but you have pyqt5 5.15.7 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "854b7191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import censusdata\n",
    "from census import Census\n",
    "from us import states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4836622e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geocodio\n",
      "  Downloading geocodio-1.0.1.tar.gz (1.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: Requests>=2.2.0 in c:\\users\\abhinaya\\anaconda3\\lib\\site-packages (from geocodio) (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\abhinaya\\anaconda3\\lib\\site-packages (from Requests>=2.2.0->geocodio) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\abhinaya\\anaconda3\\lib\\site-packages (from Requests>=2.2.0->geocodio) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abhinaya\\anaconda3\\lib\\site-packages (from Requests>=2.2.0->geocodio) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhinaya\\anaconda3\\lib\\site-packages (from Requests>=2.2.0->geocodio) (2022.9.14)\n",
      "Building wheels for collected packages: geocodio\n",
      "  Building wheel for geocodio (setup.py): started\n",
      "  Building wheel for geocodio (setup.py): finished with status 'done'\n",
      "  Created wheel for geocodio: filename=geocodio-1.0.1-py3-none-any.whl size=2543 sha256=2c74344672de6ec9bc6d29526718adc942a6af8060d16f382677ef13b5533d08\n",
      "  Stored in directory: c:\\users\\abhinaya\\appdata\\local\\pip\\cache\\wheels\\db\\53\\84\\f2ff16dbb1841adcce45ca4994e7b2c7d2dc07cf9829c88f84\n",
      "Successfully built geocodio\n",
      "Installing collected packages: geocodio\n",
      "Successfully installed geocodio-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geocodio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07b89271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googlemaps\n",
      "  Downloading googlemaps-4.10.0.tar.gz (33 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests<3.0,>=2.20.0 in c:\\users\\abhinaya\\anaconda3\\lib\\site-packages (from googlemaps) (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abhinaya\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.20.0->googlemaps) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\abhinaya\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.20.0->googlemaps) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\abhinaya\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.20.0->googlemaps) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhinaya\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.20.0->googlemaps) (2022.9.14)\n",
      "Building wheels for collected packages: googlemaps\n",
      "  Building wheel for googlemaps (setup.py): started\n",
      "  Building wheel for googlemaps (setup.py): finished with status 'done'\n",
      "  Created wheel for googlemaps: filename=googlemaps-4.10.0-py3-none-any.whl size=40717 sha256=b09d8eb6791956320fa6d25a9b4859e4dc280a20c14974b7516a861ce12e48e6\n",
      "  Stored in directory: c:\\users\\abhinaya\\appdata\\local\\pip\\cache\\wheels\\d9\\5f\\46\\54a2bdb4bcb07d3faba4463d2884865705914cc72a7b8bb5f0\n",
      "Successfully built googlemaps\n",
      "Installing collected packages: googlemaps\n",
      "Successfully installed googlemaps-4.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install googlemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0290d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googlemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac52b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 300)\n",
    "pd.set_option('display.max_columns',100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa77d6e",
   "metadata": {},
   "source": [
    "## Import combined NTU and Tweets Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de006d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhinaya\\AppData\\Local\\Temp\\ipykernel_3444\\3880552902.py:1: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_1 = pd.read_csv('df_combined.csv')\n"
     ]
    }
   ],
   "source": [
    "df_1 = pd.read_csv('df_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8e07216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(267682, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fc83df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-13</td>\n",
       "      <td>896826568269598720.0</td>\n",
       "      <td>Gert could become a quite intense post-tropica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>897012836131581952.0</td>\n",
       "      <td>Weather Street: Tropical Storm Harvey, Hurrica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>897086851709796352.0</td>\n",
       "      <td>Tropical Storm #Gert intensifying. Tropical St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>897088053407363072.0</td>\n",
       "      <td>Tropical Storm #Gert intensifying. Tropical St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>897088252326424576.0</td>\n",
       "      <td>RT YourNews15 \"Tropical Storm #Gert intensifyi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                    id  \\\n",
       "0  2017-08-13  896826568269598720.0   \n",
       "1  2017-08-14  897012836131581952.0   \n",
       "2  2017-08-14  897086851709796352.0   \n",
       "3  2017-08-14  897088053407363072.0   \n",
       "4  2017-08-14  897088252326424576.0   \n",
       "\n",
       "                                               tweet  \n",
       "0  Gert could become a quite intense post-tropica...  \n",
       "1  Weather Street: Tropical Storm Harvey, Hurrica...  \n",
       "2  Tropical Storm #Gert intensifying. Tropical St...  \n",
       "3  Tropical Storm #Gert intensifying. Tropical St...  \n",
       "4  RT YourNews15 \"Tropical Storm #Gert intensifyi...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c47906",
   "metadata": {},
   "source": [
    "## Import labeled data CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e1910a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Nb'label' is CSV from Naive Bayes classification model in previous notebook\n",
    "\n",
    "df_nb = pd.read_csv('nb_labels.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3fbb498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate Tweet dataframe and labels dataframe\n",
    "\n",
    "df = pd.concat([df_1, df_nb], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e54f880a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>nb_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-13</td>\n",
       "      <td>896826568269598720.0</td>\n",
       "      <td>Gert could become a quite intense post-tropica...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>897012836131581952.0</td>\n",
       "      <td>Weather Street: Tropical Storm Harvey, Hurrica...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>897086851709796352.0</td>\n",
       "      <td>Tropical Storm #Gert intensifying. Tropical St...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>897088053407363072.0</td>\n",
       "      <td>Tropical Storm #Gert intensifying. Tropical St...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>897088252326424576.0</td>\n",
       "      <td>RT YourNews15 \"Tropical Storm #Gert intensifyi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                    id  \\\n",
       "0  2017-08-13  896826568269598720.0   \n",
       "1  2017-08-14  897012836131581952.0   \n",
       "2  2017-08-14  897086851709796352.0   \n",
       "3  2017-08-14  897088053407363072.0   \n",
       "4  2017-08-14  897088252326424576.0   \n",
       "\n",
       "                                               tweet  nb_label  \n",
       "0  Gert could become a quite intense post-tropica...         0  \n",
       "1  Weather Street: Tropical Storm Harvey, Hurrica...         0  \n",
       "2  Tropical Storm #Gert intensifying. Tropical St...         0  \n",
       "3  Tropical Storm #Gert intensifying. Tropical St...         0  \n",
       "4  RT YourNews15 \"Tropical Storm #Gert intensifyi...         0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c747f023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    266677\n",
       "1      1005\n",
       "Name: nb_label, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Emergency labeled value counts\n",
    "\n",
    "df['nb_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b0c739",
   "metadata": {},
   "source": [
    "## Create filtered dataframe for only emergency tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09962531",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_help = df[df['nb_label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5bae9d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1005, 4)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_help.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50828fa6",
   "metadata": {},
   "source": [
    "## Import texas cities census data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3996fc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texas cities from US CensusData Population Estimates\n",
    "\n",
    "tx_cities = pd.read_csv('texas_cities.csv', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21d6fbb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Id2</th>\n",
       "      <th>Geography</th>\n",
       "      <th>April 1, 2010 - Census</th>\n",
       "      <th>April 1, 2010 - Estimates Base</th>\n",
       "      <th>Population Estimate (as of July 1) - 2010</th>\n",
       "      <th>Population Estimate (as of July 1) - 2011</th>\n",
       "      <th>Population Estimate (as of July 1) - 2012</th>\n",
       "      <th>Population Estimate (as of July 1) - 2013</th>\n",
       "      <th>Population Estimate (as of July 1) - 2014</th>\n",
       "      <th>Population Estimate (as of July 1) - 2015</th>\n",
       "      <th>Population Estimate (as of July 1) - 2016</th>\n",
       "      <th>Population Estimate (as of July 1) - 2017</th>\n",
       "      <th>Population Estimate (as of July 1) - 2018</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1620000US4800100</td>\n",
       "      <td>4800100</td>\n",
       "      <td>Abbott city, Texas</td>\n",
       "      <td>356</td>\n",
       "      <td>361</td>\n",
       "      <td>362</td>\n",
       "      <td>362</td>\n",
       "      <td>361</td>\n",
       "      <td>358</td>\n",
       "      <td>354</td>\n",
       "      <td>354</td>\n",
       "      <td>357</td>\n",
       "      <td>363</td>\n",
       "      <td>367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1620000US4800160</td>\n",
       "      <td>4800160</td>\n",
       "      <td>Abernathy city, Texas</td>\n",
       "      <td>2805</td>\n",
       "      <td>2812</td>\n",
       "      <td>2818</td>\n",
       "      <td>2833</td>\n",
       "      <td>2822</td>\n",
       "      <td>2796</td>\n",
       "      <td>2743</td>\n",
       "      <td>2725</td>\n",
       "      <td>2747</td>\n",
       "      <td>2745</td>\n",
       "      <td>2724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1620000US4801000</td>\n",
       "      <td>4801000</td>\n",
       "      <td>Abilene city, Texas</td>\n",
       "      <td>117063</td>\n",
       "      <td>117512</td>\n",
       "      <td>117806</td>\n",
       "      <td>118749</td>\n",
       "      <td>119852</td>\n",
       "      <td>119792</td>\n",
       "      <td>120647</td>\n",
       "      <td>121694</td>\n",
       "      <td>121856</td>\n",
       "      <td>122210</td>\n",
       "      <td>122999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1620000US4801108</td>\n",
       "      <td>4801108</td>\n",
       "      <td>Ackerly city, Texas</td>\n",
       "      <td>220</td>\n",
       "      <td>220</td>\n",
       "      <td>220</td>\n",
       "      <td>219</td>\n",
       "      <td>219</td>\n",
       "      <td>225</td>\n",
       "      <td>228</td>\n",
       "      <td>230</td>\n",
       "      <td>231</td>\n",
       "      <td>226</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1620000US4801240</td>\n",
       "      <td>4801240</td>\n",
       "      <td>Addison town, Texas</td>\n",
       "      <td>13056</td>\n",
       "      <td>13062</td>\n",
       "      <td>13091</td>\n",
       "      <td>13798</td>\n",
       "      <td>15199</td>\n",
       "      <td>15437</td>\n",
       "      <td>15501</td>\n",
       "      <td>15587</td>\n",
       "      <td>15516</td>\n",
       "      <td>15497</td>\n",
       "      <td>15945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Id      Id2              Geography April 1, 2010 - Census  \\\n",
       "0  1620000US4800100  4800100     Abbott city, Texas                    356   \n",
       "1  1620000US4800160  4800160  Abernathy city, Texas                   2805   \n",
       "2  1620000US4801000  4801000    Abilene city, Texas                 117063   \n",
       "3  1620000US4801108  4801108    Ackerly city, Texas                    220   \n",
       "4  1620000US4801240  4801240    Addison town, Texas                  13056   \n",
       "\n",
       "   April 1, 2010 - Estimates Base  Population Estimate (as of July 1) - 2010  \\\n",
       "0                             361                                        362   \n",
       "1                            2812                                       2818   \n",
       "2                          117512                                     117806   \n",
       "3                             220                                        220   \n",
       "4                           13062                                      13091   \n",
       "\n",
       "   Population Estimate (as of July 1) - 2011  \\\n",
       "0                                        362   \n",
       "1                                       2833   \n",
       "2                                     118749   \n",
       "3                                        219   \n",
       "4                                      13798   \n",
       "\n",
       "   Population Estimate (as of July 1) - 2012  \\\n",
       "0                                        361   \n",
       "1                                       2822   \n",
       "2                                     119852   \n",
       "3                                        219   \n",
       "4                                      15199   \n",
       "\n",
       "   Population Estimate (as of July 1) - 2013  \\\n",
       "0                                        358   \n",
       "1                                       2796   \n",
       "2                                     119792   \n",
       "3                                        225   \n",
       "4                                      15437   \n",
       "\n",
       "   Population Estimate (as of July 1) - 2014  \\\n",
       "0                                        354   \n",
       "1                                       2743   \n",
       "2                                     120647   \n",
       "3                                        228   \n",
       "4                                      15501   \n",
       "\n",
       "   Population Estimate (as of July 1) - 2015  \\\n",
       "0                                        354   \n",
       "1                                       2725   \n",
       "2                                     121694   \n",
       "3                                        230   \n",
       "4                                      15587   \n",
       "\n",
       "   Population Estimate (as of July 1) - 2016  \\\n",
       "0                                        357   \n",
       "1                                       2747   \n",
       "2                                     121856   \n",
       "3                                        231   \n",
       "4                                      15516   \n",
       "\n",
       "   Population Estimate (as of July 1) - 2017  \\\n",
       "0                                        363   \n",
       "1                                       2745   \n",
       "2                                     122210   \n",
       "3                                        226   \n",
       "4                                      15497   \n",
       "\n",
       "   Population Estimate (as of July 1) - 2018  \n",
       "0                                        367  \n",
       "1                                       2724  \n",
       "2                                     122999  \n",
       "3                                        227  \n",
       "4                                      15945  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70c244d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Geography-Cities Column to a list\n",
    "\n",
    "cities = tx_cities['Geography']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5491ca8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Abbott city, Texas\n",
       "1    Abernathy city, Texas\n",
       "2      Abilene city, Texas\n",
       "3      Ackerly city, Texas\n",
       "4      Addison town, Texas\n",
       "Name: Geography, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf550211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean City name to remove city/town suffix and State\n",
    "\n",
    "tx_city_list = []\n",
    "for city in cities: \n",
    "    city = ' '.join(city.split()[:-2]).lower()\n",
    "    tx_city_list.append(city)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43fbf97",
   "metadata": {},
   "source": [
    "## Scrape Houseton Streets from geographic.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f438b9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to scrape street names from Geographic.org\n",
    "# with state and city parameters\n",
    "\n",
    "\n",
    "def scrape_streets(state, city):\n",
    "    base = 'https://geographic.org/streetview/usa/'\n",
    "    suffix = '.html'\n",
    "    url = base + state +'/' + city + suffix\n",
    "    request = requests.get(base + state +'/' + city + suffix).text\n",
    "    soup = BeautifulSoup(request, 'lxml')\n",
    "    my_table = soup.find('span',{'class':'listspan'})\n",
    "    st_tags = my_table.findAll('a')\n",
    "\n",
    "    streets = []\n",
    "    for i in range(1,len(st_tags)):\n",
    "        streets.append(st_tags[i].string)\n",
    "    \n",
    "    return streets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "125b5e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape streets for Houston, Texas\n",
    "\n",
    "streets = scrape_streets('tx', 'houston')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b4e6b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12561"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 12.5k streets from Google street view\n",
    "\n",
    "len(streets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "262ce864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower case street names\n",
    "\n",
    "streets_low = [st.lower() for st in streets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fea1bb4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adelle st', 'adina springs ln', 'adirondack dr', 'adler dr', 'adler lake dr']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streets_low[100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b05cb85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Duplicate Street Names\n",
    "\n",
    "streets_low = list(set(streets_low))\n",
    "streets_low.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f2d89686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variable for streets to drop\n",
    "# Street inside aiport causes some issues during lookup\n",
    "\n",
    "st_to_drop = ['a ave - william p. hobby airport (hou)']\n",
    "streets_low = [x for x in streets_low if x not in st_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c821e439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Streets that have single name duplicate with \n",
    "# streets that have street suffix\n",
    "\n",
    "st_to_drop = []\n",
    "for i in range(len(streets_low)): \n",
    "    if len(streets_low[i].split()) < 2:\n",
    "        if streets_low[i].split()[0] == streets_low[i+1].split()[0]: \n",
    "            st_to_drop.append(streets_low[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9f955947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Street Duplicates from above\n",
    "\n",
    "streets_low = [x for x in streets_low if x not in st_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2424b9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12298"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many street names remaining\n",
    "\n",
    "len(streets_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "189a4b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Process Street Name List for Lookup \n",
    "# Reducing Street Name down to 1 or 2 words,without suffixes except for short words\n",
    "# Based on research, users only include suffix for short words\n",
    "\n",
    "def street_processing(street_list):\n",
    "\n",
    "    st_first  = [st.split(' ')[0] for st in street_list]  # create list of all first words\n",
    "\n",
    "    main_search = []                     # list to hold street_names used for lookup\n",
    "\n",
    "    for street in street_list: \n",
    "        first_word = street.split()[0]   # set first_word of street name to variable for test\n",
    "\n",
    "        if len(street.split()) < 2:      # if street name is only one word\n",
    "\n",
    "            if first_word[0].isdigit() == False:  # if first character is a letter\n",
    "\n",
    "                main_search.append(street)         # append the one word street name to main search list\n",
    "                #one_wd_st.append(street)           \n",
    "\n",
    "            else: \n",
    "                main_search.append(street +' '+ 'st')  # append one word digit name + 'st' to main search_list\n",
    "\n",
    "        elif len(street.split()) == 2:        # if street name is two words \n",
    "\n",
    "            second_word = street.split()[1]   # create second word variable for streets with at least 2 words\n",
    "\n",
    "\n",
    "            if len(first_word) < 6:          # if first word is less than 6 characters\n",
    "\n",
    "                #if first_word[0].isdigit() == False:   # if first character is char, this doesnt matter\n",
    "\n",
    "                main_search.append(first_word + ' ' + second_word)    # append first and second word to search\n",
    "\n",
    "            elif len(first_word) >= 6:         # if first word is at least 6 characters\n",
    "\n",
    "                if first_word[0].isdigit():   # first word starts with num\n",
    "\n",
    "                    main_search.append(first_word + ' ' + second_word) # append first and second word to search\n",
    "\n",
    "                else:                         # starts w char\n",
    "\n",
    "                    main_search.append(first_word)  # append only first word\n",
    "\n",
    "        elif len(street.split()) > 2:         # street name is more than two words\n",
    "\n",
    "            second_word = street.split()[1]   # create second word again for different if statement\n",
    "\n",
    "            main_search.append(first_word + ' ' + second_word)   # append first two words\n",
    "\n",
    "    return main_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9c60b28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execut Street Processing Function\n",
    "\n",
    "street_lookup = street_processing(streets_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8edf5c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove Duplicates from newly cleaned street list\n",
    "\n",
    "street_lookup = list(set(street_lookup))\n",
    "street_lookup.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455aaa14",
   "metadata": {},
   "source": [
    "## Emergency tweets for address extraction testing\n",
    "### Test tweets were manually pulled from twitter, for known Harvey emergency related tweets on city of Houston Twitter account during the hurricane\n",
    "### These tweets are not in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "34080c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "twt_1 = ('My 83 y.o. Parents in imminent danger at 4922 Loch Lomond #Meyerland. Water knee deep inside home.'\n",
    "         ' Mom=heart condition. Dad=Alzheimer\\'s')\n",
    "\n",
    "twt_2 = ('We are not at inches, we are at 4-5 feet in this neighborhood. Wood Shadows II 11607 Lafferty Oaks')\n",
    "\n",
    "twt_3 = ('Plz help!! brother &family stuck in Dickson. 4901 38th street'\n",
    "         ' dickinson, tx 77539 his name is Rey (409) 999-0010')\n",
    "\n",
    "twt_4 = ('911 and coast guard ring busy. 4923 Braesvalley 77096.  4 adults, one disabled teen. on 2nd floor'\n",
    "         ' Elderly couple across street trapped.')\n",
    "\n",
    "twt_5 = ('We\\'re at the Redford Square Apartments 9406 Redford Street off 45 and Edgebrook'\n",
    "         ' we have a new born baby please help us the first floor Apart')\n",
    "\n",
    "twt_6 = ('412 Texas St. South Houston TX 77587')\n",
    "\n",
    "twt_7 = ('Apt 2105 at Meyer Forest Apts, she is handicapped and she cant get out, this is in Meyerland')\n",
    "\n",
    "#twt_8 = 'How \\'bout 12\"?  Is 12\" in a home a danger to an elderly person with limited mobility? '\\\n",
    "#         '2608 Martin Street, @PasadenaTX @PasadenaPD'\n",
    "    \n",
    "twt_8 = ('How \\'bout 12\"?  Is 12\" in a home a danger to an elderly person with limited mobility?'\n",
    "         ' 2608 Martin Street, PasadenaTX PasadenaPD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f3bb3ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine tweets into list\n",
    "\n",
    "test_twts = [twt_1, twt_2,twt_3, twt_4, twt_5, twt_6, twt_7, twt_8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bd89a280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case tweets\n",
    "\n",
    "twts_low = [twt.lower() for twt in test_twts ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "02bea93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common words after numbers that are not addresses, will be used to filter tweets with\n",
    "# digits that are not address components\n",
    "\n",
    "num_excluders = ['feet', 'inches', 'people', 'adults']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bce416c",
   "metadata": {},
   "source": [
    "## Testing: Lookup street name and extract address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "21f51a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lafferty\n",
      "lafferty oaks\n",
      "shadow\n"
     ]
    }
   ],
   "source": [
    "# Test Tweet 2\n",
    "\n",
    "matches = []\n",
    "count = 0\n",
    "for street in street_lookup: \n",
    "    if street in twt_2.lower(): \n",
    "        matches.append(street)\n",
    "        print(street)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d093d4c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11607'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting Street Number\n",
    "\n",
    "twt_2_tok = twt_2.lower().split()\n",
    "\n",
    "match_idx = twt_2.lower().split().index(matches[0])\n",
    "\n",
    "if twt_2_tok[match_idx - 1].isdigit():\n",
    "    street_num = twt_2_tok[match_idx - 1]\n",
    "    \n",
    "street_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0639c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine above extractions\n",
    "\n",
    "min_address = []\n",
    "for match in matches: \n",
    "    if len(match.split()) > 1:\n",
    "        match_idx = twt_2.lower().split().index(match.split()[0])\n",
    "        if twt_2_tok[match_idx - 1].isdigit():\n",
    "            street_num = twt_2_tok[match_idx - 1]\n",
    "            min_address.append(street_num +' '+match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "545e3bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11607 lafferty oaks']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_address"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a4f7e0",
   "metadata": {},
   "source": [
    "Address extraction method above requires additional development to extract the full address. Because it was not fully complete, ultimately it was not used.\n",
    "\n",
    "For this method to be effective, we will need to include street names for other cities and townships outside of Houston which are not on the current street\n",
    "lookup list. Our dataset contains many tweets from Port Arthur and Dickinson, both outside of Houston. For tweets with those addresses, without those street\n",
    "lists we would overlook those address during the lookup and filter step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33213faf",
   "metadata": {},
   "source": [
    "## Filter Emergency (df_help) dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c072b2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>nb_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6624</th>\n",
       "      <td>2017-08-24</td>\n",
       "      <td>900740378042617984.0</td>\n",
       "      <td>Please don't forget about your pets during Tro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13617</th>\n",
       "      <td>2017-08-27</td>\n",
       "      <td>901915749261836416.0</td>\n",
       "      <td>Here's how you can help with #Tropical_Storm_H...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14131</th>\n",
       "      <td>2017-08-27</td>\n",
       "      <td>901930631034273792.0</td>\n",
       "      <td>Come on #tropicalstorm #Harvey u need to go!!!...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27074</th>\n",
       "      <td>2017-08-30</td>\n",
       "      <td>902889514028412800.0</td>\n",
       "      <td>Did you forget Harvey is a Tropical Storm and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33222</th>\n",
       "      <td>2017-08-25</td>\n",
       "      <td>901000000000000000.0</td>\n",
       "      <td>Stay Safe #Texas Evacuate if you must &amp; don't ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33564</th>\n",
       "      <td>2017-08-25</td>\n",
       "      <td>901000000000000000.0</td>\n",
       "      <td>Better hurry</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33648</th>\n",
       "      <td>2017-08-25</td>\n",
       "      <td>901000000000000000.0</td>\n",
       "      <td>In your runs to get food and water, make sure ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33848</th>\n",
       "      <td>2017-08-25</td>\n",
       "      <td>901000000000000000.0</td>\n",
       "      <td>I?ve had stronger hurricanes at Pat O? Briens,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33984</th>\n",
       "      <td>2017-08-25</td>\n",
       "      <td>901000000000000000.0</td>\n",
       "      <td>#HurricaneHarvey pls come thru. I'm tryna slee...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34346</th>\n",
       "      <td>2017-08-25</td>\n",
       "      <td>901000000000000000.0</td>\n",
       "      <td>#TRUMP will be there #HurricaneHarvey #TEXAS e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date                    id  \\\n",
       "6624   2017-08-24  900740378042617984.0   \n",
       "13617  2017-08-27  901915749261836416.0   \n",
       "14131  2017-08-27  901930631034273792.0   \n",
       "27074  2017-08-30  902889514028412800.0   \n",
       "33222  2017-08-25  901000000000000000.0   \n",
       "33564  2017-08-25  901000000000000000.0   \n",
       "33648  2017-08-25  901000000000000000.0   \n",
       "33848  2017-08-25  901000000000000000.0   \n",
       "33984  2017-08-25  901000000000000000.0   \n",
       "34346  2017-08-25  901000000000000000.0   \n",
       "\n",
       "                                                   tweet  nb_label  \n",
       "6624   Please don't forget about your pets during Tro...         1  \n",
       "13617  Here's how you can help with #Tropical_Storm_H...         1  \n",
       "14131  Come on #tropicalstorm #Harvey u need to go!!!...         1  \n",
       "27074  Did you forget Harvey is a Tropical Storm and ...         1  \n",
       "33222  Stay Safe #Texas Evacuate if you must & don't ...         1  \n",
       "33564                                       Better hurry         1  \n",
       "33648  In your runs to get food and water, make sure ...         1  \n",
       "33848  I?ve had stronger hurricanes at Pat O? Briens,...         1  \n",
       "33984  #HurricaneHarvey pls come thru. I'm tryna slee...         1  \n",
       "34346  #TRUMP will be there #HurricaneHarvey #TEXAS e...         1  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_help.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d133e235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1005, 4)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_help.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb7a5d9",
   "metadata": {},
   "source": [
    "## Check help tweets for potential address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a0129c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 'has number' function to test if string has a digit\n",
    "\n",
    "def has_num(input_str):\n",
    "     return any(char.isdigit() for char in input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "727cdf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhinaya\\AppData\\Local\\Temp\\ipykernel_3444\\1969472212.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_help['st_matches'] = pd.Series(matches, index=df_help.index)\n",
      "C:\\Users\\Abhinaya\\AppData\\Local\\Temp\\ipykernel_3444\\1969472212.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_help['bin_match'] = pd.Series(bin_match, index=df_help.index)\n"
     ]
    }
   ],
   "source": [
    "# Check for Street Name match and presence of a digit in tweet\n",
    "# Create new columns with Binary Identifier 1 for match & list of street matches\n",
    "\n",
    "matches = []\n",
    "bin_match = []\n",
    "for twt in df_help['tweet']: \n",
    "    st_match = []\n",
    "    add_ct = 0 \n",
    "    bin_ct = 0\n",
    "    \n",
    "    for street in street_lookup:\n",
    "        if street in twt.lower():\n",
    "            if has_num(twt.lower()):\n",
    "                add_ct += 1\n",
    "                st_match.append(street)\n",
    "                bin_ct = 1\n",
    "    matches.append(st_match)\n",
    "    bin_match.append(bin_ct)\n",
    "df_help['st_matches'] = pd.Series(matches, index=df_help.index)\n",
    "df_help['bin_match'] = pd.Series(bin_match, index=df_help.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bbcbfa3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>nb_label</th>\n",
       "      <th>st_matches</th>\n",
       "      <th>bin_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6624</th>\n",
       "      <td>2017-08-24</td>\n",
       "      <td>900740378042617984.0</td>\n",
       "      <td>Please don't forget about your pets during Tro...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13617</th>\n",
       "      <td>2017-08-27</td>\n",
       "      <td>901915749261836416.0</td>\n",
       "      <td>Here's how you can help with #Tropical_Storm_H...</td>\n",
       "      <td>1</td>\n",
       "      <td>[harvey, tropical]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14131</th>\n",
       "      <td>2017-08-27</td>\n",
       "      <td>901930631034273792.0</td>\n",
       "      <td>Come on #tropicalstorm #Harvey u need to go!!!...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27074</th>\n",
       "      <td>2017-08-30</td>\n",
       "      <td>902889514028412800.0</td>\n",
       "      <td>Did you forget Harvey is a Tropical Storm and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33222</th>\n",
       "      <td>2017-08-25</td>\n",
       "      <td>901000000000000000.0</td>\n",
       "      <td>Stay Safe #Texas Evacuate if you must &amp; don't ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date                    id  \\\n",
       "6624   2017-08-24  900740378042617984.0   \n",
       "13617  2017-08-27  901915749261836416.0   \n",
       "14131  2017-08-27  901930631034273792.0   \n",
       "27074  2017-08-30  902889514028412800.0   \n",
       "33222  2017-08-25  901000000000000000.0   \n",
       "\n",
       "                                                   tweet  nb_label  \\\n",
       "6624   Please don't forget about your pets during Tro...         1   \n",
       "13617  Here's how you can help with #Tropical_Storm_H...         1   \n",
       "14131  Come on #tropicalstorm #Harvey u need to go!!!...         1   \n",
       "27074  Did you forget Harvey is a Tropical Storm and ...         1   \n",
       "33222  Stay Safe #Texas Evacuate if you must & don't ...         1   \n",
       "\n",
       "               st_matches  bin_match  \n",
       "6624                   []          0  \n",
       "13617  [harvey, tropical]          1  \n",
       "14131                  []          0  \n",
       "27074                  []          0  \n",
       "33222                  []          0  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_help.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8ee9cfe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    809\n",
       "1    196\n",
       "Name: bin_match, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Street name + num matching counts\n",
    "\n",
    "df_help['bin_match'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ee1d4ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframe filtered for Address partial match\n",
    "\n",
    "df_add = df_help[df_help['bin_match']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "afb3bf57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196, 6)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_add.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcecb6e6",
   "metadata": {},
   "source": [
    "## Address Parsing Pipeline\n",
    "\n",
    "### Addresses Manually Inspected and Extracted\n",
    "\n",
    "Below addresses were pulled for testing and mapping in the following notebook prior to completion of above Parser function. These address were used in mapping for the demonstration of mapping functionality in the Presentation. Some but not all of these address overlap with the identified addresses in the above filtered dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1f8e85e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [265179, 267654, 267451, 267422, 265192, 265161, 265146, 265145]\n",
    "address = ['9407 Cranleigh Ct. Houston 77096', '4724 Amalie St Houston','3226 Ave G',\n",
    "           '8015 Serenity Court Houston, TX', '5400 Bayou Dr. Dickson, TX', \n",
    "           'Big Bend Avenue, 39th St., Port Arthur','340 West 17th St. Port Arthur, TX 77640',\n",
    "          '3605 Jimmy Johnson Blvd Apt. 1002 Port Arthur TX 77642']\n",
    "lat = ['29.6784058','29.8551516' '29.465968099', '29.6948069', '29.4494273','29.9147006', '29.8748434', \n",
    "      '29.8748434', '29.945638']\n",
    "lng = ['-95.4633444','-95.323505099',  '-95.059360599', '-95.422081499', '-95.0609667', '-93.9485301',\n",
    "      '-93.9523373999999', '-93.952337399','-93.9755919999']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "82dedff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_1 = df.loc[265179, 'tweet']\n",
    "tweet_2 = df.loc[265145, 'tweet']\n",
    "tweet_3 = df.loc[267654, 'tweet']\n",
    "tweet_4 = df.loc[267451, 'tweet']\n",
    "tweet_5 = df.loc[267422, 'tweet']\n",
    "tweet_6 = df.loc[265161, 'tweet']\n",
    "tweet_7 = df.loc[265146, 'tweet']\n",
    "tweet_8 = df.loc[265145, 'tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2562a4cf",
   "metadata": {},
   "source": [
    "## Testing Libpostal parser on raw extracted tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a49cb396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import usaddress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7c21ffb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('houstonpolice', 'Recipient'),\n",
       " ('#', 'Recipient'),\n",
       " ('HarveyRescue', 'Recipient'),\n",
       " ('#', 'Recipient'),\n",
       " ('HarveySOS', 'Recipient'),\n",
       " ('9407', 'AddressNumber'),\n",
       " ('Cranleigh', 'StreetName'),\n",
       " ('Ct.', 'StreetNamePostType'),\n",
       " ('Houston', 'SubaddressType'),\n",
       " ('77096', 'SubaddressIdentifier'),\n",
       " ('PLEASE', 'SubaddressType'),\n",
       " ('FAMILY', 'SubaddressIdentifier'),\n",
       " ('OF', 'SubaddressType'),\n",
       " ('5', 'SubaddressIdentifier'),\n",
       " ('NEED', 'SubaddressType'),\n",
       " ('HELP.', 'Recipient'),\n",
       " (\"They're\", 'Recipient'),\n",
       " ('trapped', 'Recipient'),\n",
       " ('on', 'Recipient'),\n",
       " ('the', 'Recipient'),\n",
       " ('roof', 'Recipient'),\n",
       " ('#', 'Recipient'),\n",
       " ('houston', 'Recipient')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usaddress.parse(tweet_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0d7146bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('HarveyRescue', 'Recipient'),\n",
       " ('3', 'Recipient'),\n",
       " ('adults', 'Recipient'),\n",
       " ('and', 'Recipient'),\n",
       " ('4', 'Recipient'),\n",
       " ('children', 'Recipient'),\n",
       " ('need', 'Recipient'),\n",
       " ('help.', 'Recipient'),\n",
       " ('3605', 'AddressNumber'),\n",
       " ('Jimmy', 'StreetName'),\n",
       " ('Johnson', 'StreetName'),\n",
       " ('Blvd', 'StreetNamePostType'),\n",
       " ('Apt.', 'OccupancyType'),\n",
       " ('1002', 'OccupancyIdentifier'),\n",
       " ('Port', 'PlaceName'),\n",
       " ('Arthur', 'PlaceName'),\n",
       " ('TX', 'StateName'),\n",
       " ('77642.', 'ZipCode'),\n",
       " ('#', 'Recipient'),\n",
       " ('HarveySOS', 'Recipient')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usaddress.parse(tweet_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b55398db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"I've\", 'Recipient'),\n",
       " ('got', 'Recipient'),\n",
       " ('a', 'Recipient'),\n",
       " ('scared', 'Recipient'),\n",
       " ('friend', 'Recipient'),\n",
       " ('at', 'Recipient'),\n",
       " ('4724', 'AddressNumber'),\n",
       " ('Amalie', 'StreetName'),\n",
       " ('St.', 'StreetNamePostType'),\n",
       " ('please', 'Recipient'),\n",
       " ('send', 'Recipient'),\n",
       " ('help', 'Recipient'),\n",
       " ('KHOU', 'Recipient'),\n",
       " ('houstonpolice', 'Recipient'),\n",
       " ('abc13houston', 'Recipient'),\n",
       " ('HoustonTX', 'Recipient')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usaddress.parse(tweet_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e95c8ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('My', 'Recipient'),\n",
       " ('FIL', 'Recipient'),\n",
       " ('Frank', 'Recipient'),\n",
       " ('Emmitte', 'Recipient'),\n",
       " ('is', 'Recipient'),\n",
       " ('trapped', 'Recipient'),\n",
       " ('3226', 'Recipient'),\n",
       " ('Ave', 'Recipient'),\n",
       " ('G', 'Recipient'),\n",
       " ('with', 'Recipient'),\n",
       " (\"4-5'\", 'Recipient'),\n",
       " ('water.', 'Recipient'),\n",
       " ('He', 'Recipient'),\n",
       " ('is', 'Recipient'),\n",
       " ('elderly,', 'Recipient'),\n",
       " ('cannot', 'Recipient'),\n",
       " ('walk', 'Recipient'),\n",
       " ('well.', 'Recipient'),\n",
       " ('Pls', 'Recipient'),\n",
       " ('help,', 'Recipient'),\n",
       " ('retweet,', 'Recipient'),\n",
       " ('send', 'Recipient'),\n",
       " ('info', 'Recipient'),\n",
       " ('#', 'Recipient'),\n",
       " ('HurricaneHarvery', 'Recipient')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usaddress.parse(tweet_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6733fd08",
   "metadata": {},
   "source": [
    "## Address parser function to manipulate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5be1764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function uses libpostal address parser and reformats output into a dictionary\n",
    "\n",
    "def libpost_parser(tweet):\n",
    "    libpost = usaddress.parse(tweet)\n",
    "    parsed_address = {}\n",
    "    for value, key in libpost: \n",
    "        parsed_address[key] = value\n",
    "\n",
    "    return parsed_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5e37ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function on several tweets\n",
    "\n",
    "parsed_address = libpost_parser(twt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "93aefc59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Recipient': 'II', 'AddressNumber': '11607', 'StreetName': 'Oaks'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b9f5ab4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AddressNumber': '412',\n",
       " 'StreetName': 'Texas',\n",
       " 'StreetNamePostType': 'St.',\n",
       " 'PlaceName': 'Houston',\n",
       " 'StateName': 'TX',\n",
       " 'ZipCode': '77587'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "libpost_parser(twt_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfc7b52",
   "metadata": {},
   "source": [
    "## Expand libpostal parser function\n",
    "\n",
    "Process tweets in dataframe based on multiple conditionals about the validity of output from the libpostal parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3c191d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full address parser function that asseses validity of the output from libpostal parser\n",
    "# based on city and street lookups, and returns a combined formatted address.\n",
    "\n",
    "def parser_full(tweet): \n",
    "    \n",
    "    tweet = tweet.replace('#', ' ').replace('@', ' ').replace('/', ' ')\n",
    "    libpost = usaddress.parse(tweet)\n",
    "    parsed_address = {}\n",
    "    \n",
    "    for value, key in libpost: \n",
    "        parsed_address[key] = value\n",
    "    \n",
    "    st_num_bin = 0\n",
    "    street_bin = 0\n",
    "    city_bin = 0\n",
    "    state = 0\n",
    "    zip_bin = 0\n",
    "    \n",
    "    if 'house_number' in parsed_address:\n",
    "        if parsed_address['house_number'].isdigit() and (len(parsed_address['house_number']) < 6): \n",
    "            street_num = parsed_address['house_number']\n",
    "            st_num_bin = 1\n",
    "    else: \n",
    "        street_num = 'NA'\n",
    "    \n",
    "    if 'road' in parsed_address:\n",
    "        #if parsed_address['road'] in street_lookup:\n",
    "        street = parsed_address['road']\n",
    "        street_bin = 1  \n",
    "            \n",
    "        # Below section requires further testing to validate street    \n",
    "        #elif 'city' in parsed_address:\n",
    "            #if parsed_address['city'] in tx_city_list: #and parsed_address['city'] != 'houston':\n",
    "                #street = parsed_address['road']\n",
    "                #street_bin = 1 \n",
    "            #else: \n",
    "                #street = 'NA'\n",
    "        #else: \n",
    "            #street = 'NA'\n",
    "    else: \n",
    "        street = 'NA'\n",
    "    \n",
    "    if 'city' in parsed_address:\n",
    "        if parsed_address['city'] in tx_city_list:\n",
    "            city = parsed_address['city']\n",
    "            city_bin = 1\n",
    "    #else: \n",
    "        #city = 'houston'\n",
    "        #city_bin = 1\n",
    "\n",
    "    # Below block should be used when pulling data from multiple states     \n",
    "    if 'state' in parsed_address:\n",
    "        if len(parsed_address['state']) == 2:\n",
    "            state = parsed_address['state']\n",
    "            state_bin = 1;\n",
    "    else: \n",
    "        state = 'tx'\n",
    "        state_bin = 1\n",
    "    \n",
    "    if 'postcode' in parsed_address:\n",
    "        if parsed_address['postcode'].isdigit() and (len(parsed_address['postcode']) == 5):    \n",
    "            zipcode = parsed_address['postcode']\n",
    "            zip_bin = 1\n",
    "            \n",
    "    if st_num_bin == 0 and street_bin == 0: \n",
    "        formatted_addr = 'NA'\n",
    "        \n",
    "    if street_bin == 1:\n",
    "        if st_num_bin == 1: \n",
    "            if city_bin == 1: \n",
    "                if zip_bin == 1:\n",
    "                    formatted_addr = f'{street_num} {street}, {city}, {state} {zipcode}'\n",
    "                else: \n",
    "                    formatted_addr = f'{street_num} {street}, {city}, {state}'\n",
    "            elif zip_bin == 1: \n",
    "                formatted_addr = f'{street_num} {street}, {state} {zipcode}'\n",
    "            else: \n",
    "                formatted_addr = f'{street_num} {street}, {state}'\n",
    "        else:\n",
    "            if city_bin == 1: \n",
    "                if zip_bin == 1:\n",
    "                    formatted_addr = f'{street}, {city}, {state} {zipcode}'\n",
    "                else: \n",
    "                    formatted_addr = f'{street}, {city}, {state}'\n",
    "            elif zip_bin == 1: \n",
    "                formatted_addr = f'{street}, {state} {zipcode}'  \n",
    "            else: \n",
    "                formatted_addr = 'NA'\n",
    "    else: \n",
    "        formatted_addr = 'NA'\n",
    "    \n",
    "    \n",
    "    return formatted_addr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "64e252ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NA'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test parser function\n",
    "\n",
    "parser_full(twt_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a5f6479f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Recipient': 'II', 'AddressNumber': '11607', 'StreetName': 'Oaks'}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0b6add92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Forward\\n', 'SubaddressType'),\n",
       " ('1', 'SubaddressIdentifier'),\n",
       " ('adult,', 'SubaddressType'),\n",
       " ('8', 'SubaddressIdentifier'),\n",
       " ('year', 'Recipient'),\n",
       " ('old', 'Recipient'),\n",
       " ('w/autism)\\n', 'Recipient'),\n",
       " ('On', 'Recipient'),\n",
       " ('top', 'Recipient'),\n",
       " ('of', 'Recipient'),\n",
       " ('car', 'Recipient'),\n",
       " ('in', 'Recipient'),\n",
       " ('garage\\n', 'Recipient'),\n",
       " ('2935', 'AddressNumber'),\n",
       " ('40th', 'StreetName'),\n",
       " ('St.', 'StreetNamePostType'),\n",
       " ('#', 'OccupancyIdentifier'),\n",
       " ('Port', 'PlaceName'),\n",
       " ('Arthur,', 'PlaceName'),\n",
       " ('TX', 'StateName'),\n",
       " ('77642\\n', 'ZipCode'),\n",
       " ('#', 'Recipient'),\n",
       " ('SOS', 'Recipient'),\n",
       " ('#', 'Recipient'),\n",
       " ('HELP', 'Recipient'),\n",
       " ('#', 'Recipient'),\n",
       " ('HARVEY', 'Recipient'),\n",
       " ('#', 'Recipient'),\n",
       " ('HARVEYSOS', 'Recipient')]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usaddress.parse(df.loc[265164, 'tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "33972dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NA'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser_full(df.loc[265164, 'tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f6acb8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhinaya\\AppData\\Local\\Temp\\ipykernel_3444\\3169388344.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_add['formatted_addr'] = df_add['tweet'].apply(parser_full)\n"
     ]
    }
   ],
   "source": [
    "# Apply Address Parser to entire tweet column in dataframe creating new column\n",
    "\n",
    "df_add['formatted_addr'] = df_add['tweet'].apply(parser_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "23580303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>nb_label</th>\n",
       "      <th>st_matches</th>\n",
       "      <th>bin_match</th>\n",
       "      <th>formatted_addr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date, id, tweet, nb_label, st_matches, bin_match, formatted_addr]\n",
       "Index: []"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Dataframe for non-'NA' values\n",
    "\n",
    "df_add[df_add['formatted_addr'] != 'NA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a620775e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3444\\1966064201.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Test Geocodio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mgeo_loc2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeocode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'4901 38th Street,dickinson, tx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mgeo_loc2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'results'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "# Test Geocodio\n",
    "\n",
    "geo_loc2 = client.geocode('4901 38th Street,dickinson, tx')\n",
    "geo_loc2['results'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af062787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b73ee50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af03658c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
